{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matt and Paul's Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Packages and Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, train_test_split,GridSearchCV\n",
    "from sklearn.metrics import plot_confusion_matrix, plot_roc_curve, accuracy_score,recall_score,precision_score,\\\n",
    "                            f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Test on Iris Dataset (to be deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris Data to test functions\n",
    "iris = load_iris() \n",
    "print(iris.target_names)\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Petal length and width features\n",
    "feature_used = iris.feature_names[2:]\n",
    "X = iris.data[:, 2:] \n",
    "y = iris.target \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function Builds for Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale with Standard Scaler\n",
    "def SS(X_train,X_test):\n",
    "    ss = StandardScaler()\n",
    "    return ss.fit_transform(X_train), ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to use GridSearchCV, find the best hyperparameters and then create a model with those results and values. Also print some scoring metrics to assess how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'penalty': ['l1', 'l2' ,'elasticnet'],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "    \n",
    "    # Instantiate & fit LogReg model for GridSearch\n",
    "    grid_logreg = LogisticRegression(random_state=42)\n",
    "    grid_logreg.fit(X_train, y_train)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=grid_logreg, param_grid=grid, cv=5,\n",
    "                      scoring='accuracy')\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Return best hyperparameters\n",
    "    logreg_params = gs.best_params_\n",
    "    \n",
    "    # Use best penalty from best_params\n",
    "    logreg_penalty = logreg_params['penalty']\n",
    "    print(f'Penalty: {logreg_penalty}')\n",
    "    \n",
    "    # Use best solver from best_params\n",
    "    logreg_solver = logreg_params['solver']\n",
    "    print(f'Solver: {logreg_solver}')\n",
    "    \n",
    "    # Instantiate & fit LogReg model\n",
    "    log = LogisticRegression(random_state=42, penalty=logreg_penalty, solver=logreg_solver)\n",
    "    log.fit(X_train, y_train)\n",
    "    \n",
    "    # Create prediction variable using test data\n",
    "    y_pred = log.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(log, X_train, y_train, cv=cv)\n",
    "    print(f'Mean Cross-Val Score: {cv_results.mean()}')\n",
    "    \n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy Score: {acc_score}')\n",
    "    \n",
    "#     rec_score = recall_score(y_test, y_pred)\n",
    "#     print(f'Recall Score: {rec_score}')\n",
    "    \n",
    "#     prec_score = precision_score(y_test, y_pred)\n",
    "#     print(f'Precision Score: {prec_score}')\n",
    "    \n",
    "#     f1_score = f1_score(y_test, y_pred)\n",
    "#     print(f'F1 Score: {f1_score}')\n",
    "    \n",
    "#     # Plot an ROC curve (only works with binary data)\n",
    "#     plot_roc_curve(log, X_train, y_train)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(log, X_train, y_train);    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. K-Nearest Neighbors Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to use GridSearchCV, find the best hyperparameters and then create a model with those results and values. Also print some scoring metrics to assess how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, X_test, y_train, y_test, metric='minkowski', cv=5):\n",
    "    \n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'n_neighbors': [1,3,5,7,9,11,13,15,17,19,21,23,25],\n",
    "    'metric': ['minkowski', 'manhattan'],\n",
    "    'weights': ['uniform', 'distance']}\n",
    "    \n",
    "    # Instantiate & fit KNN model for GridSearch\n",
    "    grid_knn = KNeighborsClassifier()\n",
    "    grid_knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=grid_knn, param_grid=grid, cv=5, scoring='accuracy')\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Return best hyperparameters\n",
    "    knn_params = gs.best_params_\n",
    "    \n",
    "    # Use best # of neighbors from best_params\n",
    "    knn_neighbors = knn_params['n_neighbors']\n",
    "    print(f'Number of Neighbors: {knn_neighbors}')\n",
    "    \n",
    "    # Use best metric from best_params\n",
    "    knn_metric = knn_params['metric']\n",
    "    print(f'Metric: {knn_metric}')\n",
    "    \n",
    "    # Use best weights from best_params\n",
    "    knn_weights=knn_params['weights']\n",
    "    print(f'Weights: {knn_weights}')\n",
    "    \n",
    "    # Instantiate & fit K-Nearest Neighbors model\n",
    "    knn = KNeighborsClassifier(n_neighbors=knn_neighbors, metric=knn_metric,\n",
    "                               weights=knn_weights)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Create prediction variable using test data\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(knn, X_train, y_train, cv=cv)\n",
    "    print(f'Mean Cross-Val Score: {cv_results.mean()}')\n",
    "    \n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy Score: {acc_score}')\n",
    "    \n",
    "    rec_score = recall_score(y_test, y_pred)\n",
    "    print(f'Recall Score: {rec_score}')\n",
    "    \n",
    "    prec_score = precison_score(y_test, y_pred)\n",
    "    print(f'Precision Score: {prec_score}')\n",
    "    \n",
    "    f1_score = f1_score(y_test, y_pred)\n",
    "    print(f'F1 score: {f1_score}')\n",
    "    \n",
    "    # Plot an ROC curve (only works with binary data)\n",
    "    plot_roc_curve(knn, X_train, y_train)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(knn, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GridSearchCV alternative coded by hand (not to be used)\n",
    "# knn_dict={}\n",
    "# for index in range(1,33,2):\n",
    "#     knn_model = KNeighborsClassifier(n_neighbors=index)\n",
    "#     knn_log_loss = -1 * cross_val_score(knn_model, X_train,\n",
    "#                                         y_train, scoring=\"accuracy\").mean()\n",
    "#     knn_dict[index] = knn_log_loss\n",
    "# min_knn = min(knn_dict.values())\n",
    "# low_key = list(knn_dict.keys())[list(knn_dict.values()).index(min_knn)]\n",
    "# knn = KNeighborsClassifier(n_neighbors=low_key,metric=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to use GridSearchCV, find the best hyperparameters and then create a model with those results and values. Also print some scoring metrics to assess how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtree(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'max_depth': [2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "    'min_samples_split': [2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "    'criterion': ['gini', 'entropy']}\n",
    "    \n",
    "    # Instantiate & fit Decision Tree model for GridSearch\n",
    "    grid_dt = DecisionTreeClassifier()\n",
    "    grid_dt.fit(X_train, y_train)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=grid_dt, param_grid=grid, cv=5, scoring='accuracy')\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Return best hyperparameters\n",
    "    dt_params = gs.best_params_\n",
    "    \n",
    "    # Use best max depth from best_params\n",
    "    dt_max_depth = dt_params['max_depth']\n",
    "    print(f'Max Depth: {dt_max_depth}')\n",
    "    \n",
    "    # Use best minimum sample split from best_params\n",
    "    dt_min_samp = dt_params['min_samples_split']\n",
    "    print(f'Min Sample Split: {dt_min_samp}')\n",
    "    \n",
    "    # Use best criterion from best_params\n",
    "    dt_criterion = dt_params['criterion']\n",
    "    print(f'criterion: {dt_criterion}')\n",
    "    \n",
    "    # Instantiate & fit Decision Tree model\n",
    "    dtree = DecisionTreeClassifier(max_depth=dt_max_depth, criterion=dt_criterion,\n",
    "                                   min_samples_split=dt_min_samp, random_state=42)\n",
    "    dtree.fit(X_train, y_train)\n",
    "    \n",
    "    # Create prediction variable using test data\n",
    "    y_pred = dtree.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(dtree, X_train, y_train, cv=cv)\n",
    "    print(f'Mean Cross-Val Score: {cv_results.mean()}')\n",
    "    \n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy Score: {acc_score}')\n",
    "    \n",
    "#     rec_score = recall_score(y_test, y_pred)\n",
    "#     print(f'Recall Score: {rec_score}')\n",
    "    \n",
    "#     prec_score = precison_score(y_test, y_pred)\n",
    "#     print(f'Precision Score: {prec_score}')\n",
    "    \n",
    "#     f1_score = f1_score(y_test, y_pred)\n",
    "#     print(f'F1 score: {f1_score}')\n",
    "    \n",
    "#     # Plot an ROC curve (only works with binary data)\n",
    "#     plot_roc_curve(dtree, X_train, y_train)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(dtree, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to use GridSearchCV, find the best hyperparameters and then create a model with those results and values. Also print some scoring metrics to assess how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, X_test, y_train, y_test, cv=5):\n",
    "    \n",
    "    # Create list for range of # of trees\n",
    "    n_list = list(range(50,150))\n",
    "    \n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'n_estimators': n_list,\n",
    "    'criterion': ['gini', 'entropy']}\n",
    "    \n",
    "    # Instantiate & fit Random Forest model for GridSearch\n",
    "    grid_rf = RandomForestClassifier()\n",
    "    grid_rf.fit(X_train,y_train)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=grid_rf, param_grid=grid, cv=5, scoring='accuracy')\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Return best hyperparameters\n",
    "    rf_params = gs.best_params_\n",
    "    \n",
    "    # Use best # of trees from best_params\n",
    "    rf_n_estimators = rf_params['n_estimators']\n",
    "    print(f'Number of Trees: {rf_n_estimators}')\n",
    "    \n",
    "    # Use best criterion from best_params\n",
    "    rf_criterion = rf_params['criterion']\n",
    "    print(f'criterion: {rf_criterion}')\n",
    "    \n",
    "    # Instantiate & fit Random Forest model\n",
    "    rforest = RandomForestClassifier(n_estimators=rf_n_estimators, criterion=rf_criterion,\n",
    "                                    random_state=42)\n",
    "    rforest.fit(X_train, y_train)\n",
    "    \n",
    "    # Create prediction variable using test data\n",
    "    y_pred = rforest.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(rforest, X_train, y_train, cv=cv)\n",
    "    print(f'Mean Cross-Val Score: {cv_results.mean()}')\n",
    "    \n",
    "    # Run forest score\n",
    "    score = rforest.score(X_test,y_test)\n",
    "    print(f'Random Forest Score: {score}')\n",
    "    \n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy Score: {acc_score}')\n",
    "    \n",
    "#     rec_score = recall_score(y_test, y_pred)\n",
    "#     print(f'Recall Score: {rec_score}')\n",
    "    \n",
    "#     prec_score = precison_score(y_test, y_pred)\n",
    "#     print(f'Precision Score: {prec_score}')\n",
    "    \n",
    "#     f1_score = f1_score(y_test, y_pred)\n",
    "#     print(f'F1 score: {f1_score}')\n",
    "    \n",
    "#     # Plot an ROC curve (only works with binary data)\n",
    "#     plot_roc_curve(rforest, X_train, y_train)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(rforest, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 5 minutes and 30 seconds to run\n",
    "random_forest(X_train, X_test, y_train, y_test, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2f. Bagging Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to use GridSearchCV, find the best hyperparameters and then create a model with those results and values. Also print some scoring metrics to assess how well the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagged(X_train, X_test, y_train, y_test, cv=5):\n",
    "\n",
    "    # Set GridSearchCV hyperparameters to compare & select\n",
    "    grid = {\n",
    "    'base_estimator__max_depth': [2,3,4,5,10,15],\n",
    "    'base_estimator__min_samples_split': [2,3,4,5,10,15],\n",
    "    'base_estimator__criterion': ['gini', 'entropy'],\n",
    "    'max_samples': [1,2,3,4,5],\n",
    "    'max_features': [1,2,3,4,5],\n",
    "    'n_estimators': [10,20,50,100]}\n",
    "    \n",
    "    # Instantiate & fit Bagging Classifier model for GridSearch\n",
    "    grid_bag = BaggingClassifier(DecisionTreeClassifier())\n",
    "    grid_bag.fit(X_train, y_train)\n",
    "    \n",
    "    # Instantiate & fit GridSearchCV with accuracy scoring\n",
    "    gs = GridSearchCV(estimator=grid_bag, param_grid=grid, cv=5, scoring='accuracy')\n",
    "    gs.fit(X_train, y_train)\n",
    "    \n",
    "    # Return best hyperparameters\n",
    "    bag_params = gs.best_params_\n",
    "    \n",
    "    # Use best max depth from best_params\n",
    "    bag_max_depth = bag_params['base_estimator__max_depth']\n",
    "    print(f'Dec Tree Max Depth: {bag_max_depth}')\n",
    "    \n",
    "    # Use best minimum sample split from best_params\n",
    "    bag_min_sample = bag_params['base_estimator__min_samples_split']\n",
    "    print(f'Dec Tree Min Sample Split: {bag_min_sample}')\n",
    "    \n",
    "    # Use best max depth from best_params\n",
    "    bag_criterion = bag_params['base_estimator__criterion']\n",
    "    print(f'Dec Tree Criterion: {bag_criterion}')\n",
    "    \n",
    "    # Use best max samples from best_params\n",
    "    bag_max_sample = bag_params['max_samples']\n",
    "    print(f'Bagging Max Samples: {bag_max_sample}')\n",
    "    \n",
    "    # Use best max features from best_params\n",
    "    bag_max_features = bag_params['max_features']\n",
    "    print(f'Bag Max Features: {bag_max_features}')\n",
    "    \n",
    "    # Use best max depth from best_params\n",
    "    bag_estimators = bag_params['n_estimators']\n",
    "    print(f'# of Base Estimators: {bag_estimators}')\n",
    "    \n",
    "    # Instantiate & fit Bagging Classifier model\n",
    "    bagging = BaggingClassifier(DecisionTreeClassifier(max_depth=bag_max_depth,\n",
    "                                min_samples_split=bag_min_sample, criterion=bag_criterion),\n",
    "                                max_samples=bag_max_sample, max_features=bag_max_features,\n",
    "                                n_estimators=bag_estimators, random_state=42)\n",
    "    bagging.fit(X_train, y_train)\n",
    "    \n",
    "    # Create prediction variable using test data\n",
    "    y_pred = bagging.predict(X_test)\n",
    "    \n",
    "    # Run cross-validate score with cv folds from function parameter\n",
    "    cv_results = cross_val_score(bagging, X_train, y_train, cv=cv)\n",
    "    print(f'Mean Cross-Val Score: {cv_results.mean()}')\n",
    "    \n",
    "    # Run bagging score\n",
    "    score = bagging.score(X_test, y_test)\n",
    "    print(f'Bagging Classifier Score: {score}')\n",
    "    \n",
    "    # Run and print accuracy, recall, precision and f1 scores\n",
    "    acc_score = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy Score: {acc_score}')\n",
    "    \n",
    "#     rec_score = recall_score(y_test, y_pred)\n",
    "#     print(f'Recall Score: {rec_score}')\n",
    "    \n",
    "#     prec_score = precison_score(y_test, y_pred)\n",
    "#     print(f'Precision Score: {prec_score}')\n",
    "    \n",
    "#     f1_score = f1_score(y_test, y_pred)\n",
    "#     print(f'F1 score: {f1_score}')\n",
    "    \n",
    "#     # Plot an ROC curve (only works with binary data)\n",
    "#     plot_roc_curve(bagging, X_train, y_train)\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(bagging, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagged(X_train, X_test, y_train, y_test, cv=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
